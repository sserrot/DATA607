---
title: "Sentiment Analysis"
author: "Santiago Torres"
date: "10/30/2021"
output: 
  rmdformats::robobook:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gutenbergr)
library(janeaustenr)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(reshape2)
```


# Recreate Sentiment Analysis
Following code from [tidy text mining](https://www.tidytextmining.com/sentiment.html)

## Store Chapter Information

The following code splits out every word within the Jane Austen books to identify what chapter and line they belong to.

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```


## Check for joy-tagged words

Here we look for words that are tagged as `joy` and show how often they appear in _Emma_

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

## Organize by Line Groups

Using the BING sentiment lexicon, classify each group of 80 lines in all the Austen books by sentiment. We can then plot how the sentiment changes over time.

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```


### plot

```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

## Compare Lexicons

Looking at just one book, _Pride & Prejudice_ and seeing how the different lexicons (`AFINN`, `BING`, and `NRC`) classify each word.

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")
```


### AFINN, BING and NRC

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

The trends seem consistent through the book, but the absolute values are clearly different. This is especially clear around the 75 mark where `NRC` and `AFINN` stay positive, but `BING` drops to the negative.


## Analysis

### NRC Positive / Negative
```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

### BING word counts

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


## Plot Word Count

```{r}

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


## stopwords

Stop words are words that are ignored for classification purposes. We add `miss` to our stop words because it is being classified as _negative_ when it's simply a title for a name.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

## Wordcloud

Most commonly found stopwords within the text.

```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Word clouds by sentiment using `BING` lexicon
```{r}
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```


## Analysis by Sentences

```{r}
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
```


## Chapters

Here we count the number of chapters in each book in order to later classify a chapter as `positive` or `negative`

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

### Negative Chapters

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```

Silge, Julia, and David Robinson. “Text Mining With R.” Tidy Text Mining, O’Reilly, 18 July 2017, www.tidytextmining.com/sentiment.html.

# Analyzing Different Corpus

I want to analyze the sentiment on Henry David Thoreau and a selection of three books freely available from the Gutenburg Project

  + [Walden, and On The Duty Of Civil Disobedience](https://www.gutenberg.org/ebooks/205)
  + [Canoeing in the Wilderness](https://www.gutenberg.org/ebooks/34990)
  + [Walking](https://www.gutenberg.org/ebooks/1022)


## Download Books
```{r}
canoe <- gutenberg_download(34990) # "Canoeing in the wilderness"
walden <- gutenberg_download(205) #Walden, and On The Duty Of Civil Disobedience
walking <- gutenberg_download(1022) #Walking
```


## Data Transformation

convert book data into a set of line numbers and chapters

```{r}
canoe <- canoe %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

walden <- walden %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

walking <- walking %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```


## Get Sentiment

```{r}
canoe_sentiment <- canoe %>%
    inner_join(get_sentiments("nrc")) %>%
    mutate(book = "Canoe")


walden_sentiment <- walden %>%
    inner_join(get_sentiments("nrc")) %>%
    mutate(book = "Walden") 

walking_sentiment <- walking %>%
    inner_join(get_sentiments("nrc")) %>%
    mutate(book = "Walking")

#combine

thoreau_sentiments <- rbind(canoe_sentiment,walden_sentiment,walking_sentiment)
```

## Plot

```{r}
canoe_sentiment %>% ggplot(aes(x=sentiment)) + geom_histogram(stat = "count")
walden_sentiment %>% ggplot(aes(x=sentiment)) + geom_histogram(stat = "count")
walking_sentiment %>% ggplot(aes(x=sentiment)) + geom_histogram(stat = "count")



thoreau_sentiments %>% ggplot(aes(fill=sentiment,x=book)) + geom_bar(position = "fill")
```

Would be easier to hone in on a few key emotions that pop up the most

```{r}
thoreau_sentiments %>% filter(sentiment %in% c("fear","joy","anticipation","trust")) %>% ggplot(aes(fill=sentiment,x=book)) + geom_bar(position = "fill")
```

## conclusion, findings, and recommendations


### Outcomes

It's clear there is more fear and anticipation in the Canoeing in the wilderness book and the least amount of fear exists in the Walking book. This intuitively makes sense since there is less risk walking versus canoeing in the wilderness. 



### Lexicon Comparison

I felt the most useful lexicon was the NRC lexicon because it gave a broader range of sentiments compared to BING and AFINN. While the numerical ranges would be nice to get from AFINN, I wanted to see the proportion of different emotions that existed in the three Walden books.

### Recommendations

We could do more statistical analysis and check if the differences in types of emotions is statistically significant. We could also check the other lexicons and compare if the positive / negative outcomes occur in a similar fashion to what happened with the Jane Austen books. Henry David Thoreau wrote his book not too much later than Jane Austen so some of the potential linguistic changes that have occured since they may not be appropriately represented in the lexicons.



```
